{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.neighbors.kde import KernelDensity\n",
    "from sklearn.metrics import precision_recall_fscore_support as prf, accuracy_score\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as D\n",
    "import torchvision.transforms as T\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import math\n",
    "import argparse\n",
    "import pprint\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KDD99Loader(object):\n",
    "    def __init__(self, data_path, N_train , mode=\"train\"):\n",
    "        self.mode=mode\n",
    "        data = np.load(data_path)\n",
    "\n",
    "        labels = data[\"kdd\"][:,-1]\n",
    "        features = data[\"kdd\"][:,:-1]\n",
    "        N, D = features.shape\n",
    "        \n",
    "        normal_data = features[labels==1]\n",
    "        normal_labels = labels[labels==1]\n",
    "\n",
    "        N_normal = normal_data.shape[0]\n",
    "\n",
    "        attack_data = features[labels==0]\n",
    "        attack_labels = labels[labels==0]\n",
    "\n",
    "        N_attack = attack_data.shape[0]\n",
    "\n",
    "        randIdx = np.arange(N_attack)\n",
    "        np.random.shuffle(randIdx)\n",
    "        self.N_train = N_train\n",
    "        \n",
    "        self.train = attack_data[randIdx[:self.N_train]]\n",
    "        self.train_labels = attack_labels[randIdx[:self.N_train]]\n",
    "        \n",
    "        \n",
    "        self.test = attack_data[randIdx[self.N_train:]]\n",
    "        self.test_labels = attack_labels[randIdx[self.N_train:]]\n",
    "\n",
    "        self.test = np.concatenate((self.test, normal_data),axis=0)\n",
    "        self.test_labels = np.concatenate((self.test_labels, normal_labels),axis=0)\n",
    "       # self.test = np.concatenate((normal_data, normal_data),axis=0)\n",
    "       # self.test_labels = np.concatenate((normal_labels, normal_labels),axis=0)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Number of images in the object dataset.\n",
    "        \"\"\"\n",
    "        if self.mode == \"train\":\n",
    "            return self.train.shape[0]\n",
    "        else:\n",
    "            return self.test.shape[0]\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.mode == \"train\":\n",
    "            return np.float32(self.train[index]), np.float32(self.train_labels[index])\n",
    "        else:\n",
    "            return np.float32(self.test[index]), np.float32(self.test_labels[index])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(data_path, batch_size, N_train, mode='train'):\n",
    "    \"\"\"Build and return data loader.\"\"\"\n",
    "    \n",
    "    dataset = KDD99Loader(data_path, N_train, mode)\n",
    "\n",
    "    shuffle = False\n",
    "    if mode == 'train':\n",
    "        shuffle = True\n",
    "\n",
    "    data_loader = DataLoader(dataset=dataset,\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=shuffle)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(input_size, hidden_size, n_hidden, input_order='sequential', input_degrees=None):\n",
    "    # MADE paper sec 4:\n",
    "    # degrees of connections between layers -- ensure at most in_degree - 1 connections\n",
    "    degrees = []\n",
    "\n",
    "    # set input degrees to what is provided in args (the flipped order of the previous layer in a stack of mades);\n",
    "    # else init input degrees based on strategy in input_order (sequential or random)\n",
    "    if input_order == 'sequential':\n",
    "        degrees += [torch.arange(input_size)] if input_degrees is None else [input_degrees]\n",
    "        for _ in range(n_hidden + 1):\n",
    "            degrees += [torch.arange(hidden_size) % (input_size - 1)]\n",
    "        degrees += [torch.arange(input_size) % input_size - 1] if input_degrees is None else [input_degrees % input_size - 1]\n",
    "\n",
    "    elif input_order == 'random':\n",
    "        degrees += [torch.randperm(input_size)] if input_degrees is None else [input_degrees]\n",
    "        for _ in range(n_hidden + 1):\n",
    "            min_prev_degree = min(degrees[-1].min().item(), input_size - 1)\n",
    "            degrees += [torch.randint(min_prev_degree, input_size, (hidden_size,))]\n",
    "        min_prev_degree = min(degrees[-1].min().item(), input_size - 1)\n",
    "        degrees += [torch.randint(min_prev_degree, input_size, (input_size,)) - 1] if input_degrees is None else [input_degrees - 1]\n",
    "\n",
    "    # construct masks\n",
    "    masks = []\n",
    "    for (d0, d1) in zip(degrees[:-1], degrees[1:]):\n",
    "        masks += [(d1.unsqueeze(-1) >= d0.unsqueeze(0)).float()]\n",
    "\n",
    "    return masks, degrees[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLinear(nn.Linear):\n",
    "    \"\"\" MADE building block layer \"\"\"\n",
    "    def __init__(self, input_size, n_outputs, mask, cond_label_size=None):\n",
    "        super().__init__(input_size, n_outputs)\n",
    "\n",
    "        self.register_buffer('mask', mask)\n",
    "\n",
    "        self.cond_label_size = cond_label_size\n",
    "        if cond_label_size is not None:\n",
    "            self.cond_weight = nn.Parameter(torch.rand(n_outputs, cond_label_size) / math.sqrt(cond_label_size))\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        out = F.linear(x, self.weight * self.mask, self.bias)\n",
    "        if y is not None:\n",
    "            out = out + F.linear(y, self.cond_weight)\n",
    "        return out\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return 'in_features={}, out_features={}, bias={}'.format(\n",
    "            self.in_features, self.out_features, self.bias is not None\n",
    "        ) + (self.cond_label_size != None) * ', cond_features={}'.format(self.cond_label_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MADE(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_hidden, cond_label_size=None, activation='relu', input_order='sequential', input_degrees=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_size -- scalar; dim of inputs\n",
    "            hidden_size -- scalar; dim of hidden layers\n",
    "            n_hidden -- scalar; number of hidden layers\n",
    "            activation -- str; activation function to use\n",
    "            input_order -- str or tensor; variable order for creating the autoregressive masks (sequential|random)\n",
    "                            or the order flipped from the previous layer in a stack of mades\n",
    "            conditional -- bool; whether model is conditional\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # base distribution for calculation of log prob under the model\n",
    "        self.register_buffer('base_dist_mean', torch.zeros(input_size))\n",
    "        self.register_buffer('base_dist_var', torch.ones(input_size))\n",
    "\n",
    "        # create masks\n",
    "        masks, self.input_degrees = create_masks(input_size, hidden_size, n_hidden, input_order, input_degrees)\n",
    "\n",
    "        # setup activation\n",
    "        if activation == 'relu':\n",
    "            activation_fn = nn.ReLU()\n",
    "        elif activation == 'tanh':\n",
    "            activation_fn = nn.Tanh()\n",
    "        else:\n",
    "            raise ValueError('Check activation function.')\n",
    "\n",
    "        # construct model\n",
    "        self.net_input = MaskedLinear(input_size, hidden_size, masks[0], cond_label_size)\n",
    "        self.net = []\n",
    "        for m in masks[1:-1]:\n",
    "            self.net += [activation_fn, MaskedLinear(hidden_size, hidden_size, m)]\n",
    "        self.net += [activation_fn, MaskedLinear(hidden_size, 2 * input_size, masks[-1].repeat(2,1))]\n",
    "        self.net = nn.Sequential(*self.net)\n",
    "\n",
    "    @property\n",
    "    def base_dist(self):\n",
    "        return D.Normal(self.base_dist_mean, self.base_dist_var)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        # MAF eq 4 -- return mean and log std\n",
    "        m, loga = self.net(self.net_input(x, y)).chunk(chunks=2, dim=1)\n",
    "        u = (x - m) * torch.exp(-loga)\n",
    "        # MAF eq 5\n",
    "        log_abs_det_jacobian = - loga\n",
    "        return u, log_abs_det_jacobian\n",
    "\n",
    "    def inverse(self, u, y=None, sum_log_abs_det_jacobians=None):\n",
    "        # MAF eq 3\n",
    "        D = u.shape[1]\n",
    "        x = torch.zeros_like(u)\n",
    "        # run through reverse model\n",
    "        for i in self.input_degrees:\n",
    "            m, loga = self.net(self.net_input(x, y)).chunk(chunks=2, dim=1)\n",
    "            x[:,i] = u[:,i] * torch.exp(loga[:,i]) + m[:,i]\n",
    "        log_abs_det_jacobian = loga\n",
    "        return x, log_abs_det_jacobian\n",
    "\n",
    "    def log_prob(self, x, y=None):\n",
    "        u, log_abs_det_jacobian = self.forward(x, y)\n",
    "        return torch.sum(self.base_dist.log_prob(u) + log_abs_det_jacobian, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FlowSequential(nn.Sequential):\n",
    "    \"\"\" Container for layers of a normalizing flow \"\"\"\n",
    "    def forward(self, x, y):\n",
    "        sum_log_abs_det_jacobians = 0\n",
    "        for module in self:\n",
    "            x, log_abs_det_jacobian = module(x, y)\n",
    "            sum_log_abs_det_jacobians = sum_log_abs_det_jacobians + log_abs_det_jacobian\n",
    "        return x, sum_log_abs_det_jacobians\n",
    "\n",
    "    def inverse(self, u, y):\n",
    "        sum_log_abs_det_jacobians = 0\n",
    "        for module in reversed(self):\n",
    "            u, log_abs_det_jacobian = module.inverse(u, y)\n",
    "            sum_log_abs_det_jacobians = sum_log_abs_det_jacobians + log_abs_det_jacobian\n",
    "        return u, sum_log_abs_det_jacobians\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(nn.Module):\n",
    "    \"\"\" RealNVP BatchNorm layer \"\"\"\n",
    "    def __init__(self, input_size, momentum=0.9, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.momentum = momentum\n",
    "        self.eps = eps\n",
    "\n",
    "        self.log_gamma = nn.Parameter(torch.zeros(input_size))\n",
    "        self.beta = nn.Parameter(torch.zeros(input_size))\n",
    "\n",
    "        self.register_buffer('running_mean', torch.zeros(input_size))\n",
    "        self.register_buffer('running_var', torch.ones(input_size))\n",
    "\n",
    "    def forward(self, x, cond_y=None):\n",
    "        if self.training:\n",
    "            self.batch_mean = x.mean(0)\n",
    "            self.batch_var = x.var(0) # note MAF paper uses biased variance estimate; ie x.var(0, unbiased=False)\n",
    "\n",
    "            # update running mean\n",
    "            self.running_mean.mul_(self.momentum).add_(self.batch_mean.data * (1 - self.momentum))\n",
    "            self.running_var.mul_(self.momentum).add_(self.batch_var.data * (1 - self.momentum))\n",
    "\n",
    "            mean = self.batch_mean\n",
    "            var = self.batch_var\n",
    "        else:\n",
    "            mean = self.running_mean\n",
    "            var = self.running_var\n",
    "\n",
    "        # compute normalized input (cf original batch norm paper algo 1)\n",
    "        x_hat = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        y = self.log_gamma.exp() * x_hat + self.beta\n",
    "\n",
    "        # compute log_abs_det_jacobian (cf RealNVP paper)\n",
    "        log_abs_det_jacobian = self.log_gamma - 0.5 * torch.log(var + self.eps)\n",
    "#        print('in sum log var {:6.3f} ; out sum log var {:6.3f}; sum log det {:8.3f}; mean log_gamma {:5.3f}; mean beta {:5.3f}'.format(\n",
    "#            (var + self.eps).log().sum().data.numpy(), y.var(0).log().sum().data.numpy(), log_abs_det_jacobian.mean(0).item(), self.log_gamma.mean(), self.beta.mean()))\n",
    "        return y, log_abs_det_jacobian.expand_as(x)\n",
    "\n",
    "    def inverse(self, y, cond_y=None):\n",
    "        if self.training:\n",
    "            mean = self.batch_mean\n",
    "            var = self.batch_var\n",
    "        else:\n",
    "            mean = self.running_mean\n",
    "            var = self.running_var\n",
    "\n",
    "        x_hat = (y - self.beta) * torch.exp(-self.log_gamma)\n",
    "        x = x_hat * torch.sqrt(var + self.eps) + mean\n",
    "\n",
    "        log_abs_det_jacobian = 0.5 * torch.log(var + self.eps) - self.log_gamma\n",
    "\n",
    "        return x, log_abs_det_jacobian.expand_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAF(nn.Module):\n",
    "    def __init__(self, n_blocks, input_size, hidden_size, n_hidden, cond_label_size=None, activation='relu', input_order='sequential', batch_norm=False):\n",
    "        super().__init__()\n",
    "        # base distribution for calculation of log prob under the model\n",
    "        self.register_buffer('base_dist_mean', torch.zeros(input_size))\n",
    "        self.register_buffer('base_dist_var', torch.ones(input_size))\n",
    "\n",
    "        # construct model\n",
    "        modules = []\n",
    "        self.input_degrees = None\n",
    "        for i in range(n_blocks):\n",
    "            modules += [MADE(input_size, hidden_size, n_hidden, cond_label_size, activation, input_order, self.input_degrees)]\n",
    "            self.input_degrees = modules[-1].input_degrees.flip(0)\n",
    "            modules += batch_norm * [BatchNorm(input_size)]\n",
    "\n",
    "        self.net = FlowSequential(*modules)\n",
    "\n",
    "    @property\n",
    "    def base_dist(self):\n",
    "        return D.Normal(self.base_dist_mean, self.base_dist_var)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        return self.net(x, y)\n",
    "\n",
    "    def inverse(self, u, y=None):\n",
    "        return self.net.inverse(u, y)\n",
    "\n",
    "    def log_prob(self, x, y=None):\n",
    "        u, sum_log_abs_det_jacobians = self.forward(x, y)\n",
    "        return torch.sum(self.base_dist.log_prob(u) + sum_log_abs_det_jacobians, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 进行密度+重建误差试验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n",
      "[0.98416182 0.98693564 0.99346806 0.99019108]\n",
      "0.02\n",
      "[0.98676424 0.98952409 0.99410828 0.99181088]\n",
      "0.03\n",
      "[0.98681743 0.99054529 0.99316182 0.99185183]\n",
      "0.04\n",
      "[0.98619346 0.99114747 0.991802   0.99147462]\n",
      "0.05\n",
      "[0.98257925 0.98396114 0.99473589 0.98931918]\n",
      "0.06\n",
      "[0.98254145 0.98489799 0.9937554  0.98930687]\n",
      "0.07\n",
      "[0.98353694 0.98649359 0.99338363 0.98992662]\n",
      "0.08\n",
      "[0.98388858 0.98770177 0.99261361 0.9901516 ]\n"
     ]
    }
   ],
   "source": [
    "data_path = 'kdd_cup.npz'\n",
    "learn_rate = 0.0001\n",
    "All_train = 97278\n",
    "Ratio = 0.01\n",
    "N_train = int(All_train * Ratio)\n",
    "\n",
    "result = []\n",
    "diff_quantity_result= []\n",
    "n_blocks = 1\n",
    "n_hidden = 4\n",
    "\n",
    "\n",
    "Average_cycle = 2\n",
    "input_size = 118\n",
    "hidden_size = 118\n",
    "cond_label_size = 1\n",
    "for i in range (8):\n",
    "    N_train = int(All_train*Ratio*(i+1))\n",
    "    result = []\n",
    "    print(Ratio*(i+1))\n",
    "    for i in range(Average_cycle):\n",
    "        maf = MAF(n_blocks,input_size, hidden_size, n_hidden, cond_label_size=1, activation='relu', input_order='sequential', batch_norm=False)\n",
    "        loss_function=nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(maf.parameters(),lr=learn_rate)\n",
    "        batch_size = 64\n",
    "        data_loader_train = get_loader(data_path, batch_size, N_train, mode='train')\n",
    "        for i in range(150):\n",
    "            for j ,(input_data, labels)  in enumerate(data_loader_train):\n",
    "                optimizer.zero_grad()\n",
    "                u, log_abs_det_jacobian = maf(input_data)\n",
    "                loss_1 = loss_function(input_data,u)\n",
    "                loss_2 = - maf.log_prob(input_data).mean(0)\n",
    "                loss = loss_1 + loss_2\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "\n",
    "        batch_size = 1\n",
    "        data_loader_test = get_loader(data_path, batch_size, N_train, mode='test')\n",
    "        test_loss = []\n",
    "        test_labels = []\n",
    "        \n",
    "        for i ,(input_data, labels)  in enumerate(data_loader_test):\n",
    "            u, log_abs_det_jacobian = maf(input_data)\n",
    "            loss_1 = loss_function(input_data,u)\n",
    "            loss_2 = - maf.log_prob(input_data).mean(0)\n",
    "            loss = loss_1 + loss_2\n",
    "            loss = loss.detach().numpy()\n",
    "            test_loss.append(loss)\n",
    "            test_labels.append(labels.numpy())\n",
    "        test_labels = np.concatenate(test_labels,axis=0)\n",
    "        test_score = test_loss\n",
    "        s = len(test_labels)\n",
    "        c = np.sum(test_labels==1)\n",
    "        g = c/s\n",
    "        \n",
    "        thresh = np.percentile(test_score, int((1-g)*100))\n",
    "        pred = (test_score > thresh).astype(int)\n",
    "        gt = test_labels.astype(int)\n",
    "        accuracy = accuracy_score(gt,pred)\n",
    "        precision, recall, f_score, support = prf(gt, pred, average='binary')\n",
    "        temp_result = [accuracy,precision,recall,f_score]\n",
    "        result.append(temp_result)\n",
    "    end_result = np.mean(result,axis=0)\n",
    "    diff_quantity_result.append(end_result)\n",
    "    print(end_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
